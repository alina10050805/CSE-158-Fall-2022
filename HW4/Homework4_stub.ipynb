{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "f = gzip.open(\"young_adult_20000.json.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)\n",
    "    if len(dataset) >= 20000:\n",
    "        break\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'dc3763cdb9b2cae805882878eebb6a32',\n",
       " 'book_id': '18471619',\n",
       " 'review_id': '66b2ba840f9bd36d6d27f46136fe4772',\n",
       " 'rating': 3,\n",
       " 'review_text': 'Sherlock Holmes and the Vampires of London \\n Release Date: April 2014 \\n Publisher: Darkhorse Comics \\n Story by: Sylvain Cordurie \\n Art by: Laci \\n Colors by: Axel Gonzabo \\n Cover by: Jean Sebastien Rossbach \\n ISDN: 9781616552664 \\n MSRP: $17.99 Hardcover \\n \"Sherlock Holmes died fighting Professor Moriarty in the Reichenbach Falls. \\n At least, that\\'s what the press claims. \\n However, Holmes is alive and well and taking advantage of his presumed death to travel the globe. \\n Unfortunately, Holmes\\'s plans are thwarted when a plague of vampirism haunts Britain. \\n This book collects Sherlock Holmes and the Vampires of London Volumes 1 and 2, originally created by French publisher Soleil.\" - Darkhorse Comics \\n When I received this copy of \"Sherlock Holmes and the Vampires of London\" I was Ecstatic! The cover art was awesome and it was about two of my favorite things, Sherlock Holmes and Vampires. I couldn\\'t wait to dive into this! \\n Unfortunately, that is where my excitement ended. The story takes place a month after Sherlock Holmes supposed death in his battle with Professor Moriarty. Sherlock\\'s plan to stay hidden and out of site are ruined when on a trip with his brother Mycroft, they stumble on the presence of vampires. That is about as much of Sherlock\\'s character that comes through the book. I can\\'t even tell you the story really because nothing and I mean nothing stuck with me after reading it. I never, ever got the sense of Sherlock Holmes anywhere in this graphic novel, nor any real sense of mystery or crime. It was just Sherlock somehow battling vampires that should have had absolutely no trouble snuffing him out in a fight, but somehow always surviving and holding his own against supernatural, super powerful, blazingly fast creatures. \\n The cover art is awesome and it truly made me excited to read this but everything else feel completely flat for me. I tried telling myself that \"it\\'s a graphic novel, it would be hard to translate mystery, details, emotion\" but then I remembered reading DC Comic\\'s \"Identity Crisis\" and realized that was a load of crap. I know it\\'s unfair to compare the two as \"Identity Crisis\" had popular mystery author Brad Meltzer writing it right? Yeah....no. The standard was set that day and there is more than enough talent out there to create a great story in a graphic novel. \\n That being said, it wasn\\'t a horrible story, it just didn\\'t grip me for feel anything like Sherlock Holmes to me. It was easy enough to follow but I felt no sense of tension, stakes or compassion for any of the characters. \\n As far as the vampires go, it\\'s hard to know what to expect anymore as there are so many different versions these days. This was the more classic version which I personally prefer, but again I didn\\'t find anything that portrayed their dominance, calm confidence or sexuality. There was definitely a presence of their physical prowess but somehow that was lost on me as easily as Sherlock was able to defend himself. I know it, wouldn\\'t do to kill of the main character, but this would have a been a great opportunity to build around the experience and beguiling nature of a vampire that had lived so many years of experience. Another chance to showcase Sherlock\\'s intellect in a battle of wits over strength in something more suitable for this sort of story as apposed to trying to make it feel like an action movie. \\n Maybe I expected to much and hoped to have at least a gripping premise or some sort of interesting plot or mystery but I didn\\'t find it here. This may be a must have for serious Sherlock Holmes fans that have to collect everything about him, but if you are looking for a great story inside a graphic novel, I would have to say pass on this one. \\n That artwork is good, cover is great, story is lacking so I am giving it 2.5 out of 5 stars.',\n",
       " 'date_added': 'Thu Dec 05 10:44:25 -0800 2013',\n",
       " 'date_updated': 'Thu Dec 05 10:45:15 -0800 2013',\n",
       " 'read_at': 'Tue Nov 05 00:00:00 -0800 2013',\n",
       " 'started_at': '',\n",
       " 'n_votes': 0,\n",
       " 'n_comments': 0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommonUnigrams():\n",
    "    wordCount = defaultdict(int)\n",
    "    punctuation = set(string.punctuation)\n",
    "    for d in dataset:\n",
    "        r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "        ws = r.split()\n",
    "        for w in ws:\n",
    "            wordCount[w] += 1\n",
    "\n",
    "    counts = [(wordCount[w], w) for w in wordCount]\n",
    "    counts.sort()\n",
    "    counts.reverse()\n",
    "\n",
    "    words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "    wordId = dict(zip(words, range(len(words))))\n",
    "    wordSet = set(words)\n",
    "\n",
    "    def feature(datum):\n",
    "        feat = [0]*len(words)\n",
    "        r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "        ws = r.split()\n",
    "        for w in ws:\n",
    "            if w in words:\n",
    "                feat[wordId[w]] += 1\n",
    "        feat.append(1) #offset\n",
    "        return feat\n",
    "\n",
    "    X = [feature(d) for d in dataset]\n",
    "    y = [d['rating'] for d in dataset]\n",
    "\n",
    "    Xtrain = X[:10000]\n",
    "    Xtext = X[10000:]\n",
    "    ytrain = y[:10000]\n",
    "    ytest = y[10000:]\n",
    "\n",
    "    clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(Xtext)\n",
    "\n",
    "    wordSort = list(zip(theta[:-1], words))\n",
    "    wordSort.sort()\n",
    "\n",
    "    mse = sum((ytest - predictions)**2)/len(ytest)\n",
    "\n",
    "    return mse, wordSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommonBigrams():\n",
    "    wordCount = defaultdict(int)\n",
    "    punctuation = set(string.punctuation)\n",
    "    for d in dataset:\n",
    "        r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "        ws = r.split()\n",
    "        ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "        for w in ws2:\n",
    "            wordCount[w] += 1\n",
    "\n",
    "    counts = [(wordCount[w], w) for w in wordCount]\n",
    "    counts.sort()\n",
    "    counts.reverse()\n",
    "\n",
    "    words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "    wordId = dict(zip(words, range(len(words))))\n",
    "    wordSet = set(words)\n",
    "\n",
    "    def feature(datum):\n",
    "        feat = [0]*len(words)\n",
    "        r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "        ws = r.split()\n",
    "        ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "        for w in ws2:\n",
    "            if w in words:\n",
    "                feat[wordId[w]] += 1\n",
    "        feat.append(1) #offset\n",
    "        return feat\n",
    "\n",
    "    X = [feature(d) for d in dataset]\n",
    "    y = [d['rating'] for d in dataset]\n",
    "\n",
    "    Xtrain = X[:10000]\n",
    "    Xtext = X[10000:]\n",
    "    ytrain = y[:10000]\n",
    "    ytest = y[10000:]\n",
    "\n",
    "    clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(Xtext)\n",
    "\n",
    "    wordSort = list(zip(theta[:-1], words))\n",
    "    wordSort.sort()\n",
    "\n",
    "    mse = sum((ytest - predictions)**2)/len(ytest)\n",
    "\n",
    "    return mse, wordSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommonBoth():\n",
    "    wordCount = defaultdict(int)\n",
    "    punctuation = set(string.punctuation)\n",
    "    for d in dataset:\n",
    "        r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "        ws = r.split()\n",
    "        ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "        for w in ws + ws2:\n",
    "            wordCount[w] += 1\n",
    "\n",
    "    counts = [(wordCount[w], w) for w in wordCount]\n",
    "    counts.sort()\n",
    "    counts.reverse()\n",
    "\n",
    "    words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "    wordId = dict(zip(words, range(len(words))))\n",
    "    wordSet = set(words)\n",
    "\n",
    "    def feature(datum):\n",
    "        feat = [0]*len(words)\n",
    "        r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "        ws = r.split()\n",
    "        ws2 = [' '.join(x) for x in list(zip(ws[:-1],ws[1:]))]\n",
    "        for w in ws + ws2:\n",
    "            if w in words:\n",
    "                feat[wordId[w]] += 1\n",
    "        feat.append(1) #offset\n",
    "        return feat\n",
    "\n",
    "    X = [feature(d) for d in dataset]\n",
    "    y = [d['rating'] for d in dataset]\n",
    "\n",
    "    Xtrain = X[:10000]\n",
    "    Xtext = X[10000:]\n",
    "    ytrain = y[:10000]\n",
    "    ytest = y[10000:]\n",
    "\n",
    "    clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "    clf.fit(Xtrain, ytrain)\n",
    "    theta = clf.coef_\n",
    "    predictions = clf.predict(Xtext)\n",
    "\n",
    "    wordSort = list(zip(theta[:-1], words))\n",
    "    wordSort.sort()\n",
    "\n",
    "    mse = sum((ytest - predictions)**2)/len(ytest)\n",
    "\n",
    "    return mse, wordSort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2390553477075859, ['boring', 'disappointing', 'says', 'worst', 'basically'], ['5', 'yourself', 'beautifully', 'mix', 'wait']]\n",
      "[1.2930626118603759, ['tuned for', 'miss your', 'the worst', 'a bad', 'too many'], ['reviews as', '5 stars', 'stay tuned', 'cant wait', 'forget to']]\n",
      "[1.2366939869514826, ['katies corner', 'share', 'what is', 'least', 'able to'], ['at least', 'excellent', 'wait', 'able', 'katies']]\n"
     ]
    }
   ],
   "source": [
    "for q,wList in ('Q1a', mostCommonUnigrams), ('Q1b', mostCommonBigrams), ('Q1c', mostCommonBoth):\n",
    "    mse, wordSort = wList()\n",
    "\n",
    "    answers[q] = [float(mse), [x[1] for x in wordSort[:5]], [x[1] for x in wordSort[-5:]]]\n",
    "\n",
    "print(answers['Q1a'])\n",
    "print(answers['Q1b'])\n",
    "print(answers['Q1c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in 'Q1a', 'Q1b', 'Q1c':\n",
    "    assert len(answers[q]) == 3\n",
    "    assertFloat(answers[q][0])\n",
    "    assert [type(x) for x in answers[q][1]] == [str]*5\n",
    "    assert [type(x) for x in answers[q][2]] == [str]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in dataset:\n",
    "    r = d['review_text']\n",
    "    ws = r.split()\n",
    "    for w in ws:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "df = defaultdict(int)\n",
    "for d in dataset:\n",
    "    r = d['review_text']\n",
    "    for w in set(r.split()):\n",
    "        df[w] += 1\n",
    "\n",
    "rev = dataset[9]\n",
    "\n",
    "tf = defaultdict(int)\n",
    "r = rev['review_text']\n",
    "for w in r.split():\n",
    "    # Note = rather than +=, different versions of tf could be used instead\n",
    "    tf[w] = 1\n",
    "    \n",
    "tfidf = dict(zip(words,[tf[w] * math.log2(len(dataset) / df[w]) for w in words]))\n",
    "tfidfQuery = [tf[w] * math.log2(len(dataset) / df[w]) for w in words]\n",
    "\n",
    "def Cosine(x1,x2):\n",
    "    numer = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for a1,a2 in zip(x1,x2):\n",
    "        numer += a1*a2\n",
    "        norm1 += a1**2\n",
    "        norm2 += a2**2\n",
    "    if norm1*norm2:\n",
    "        return numer / math.sqrt(norm1*norm2)\n",
    "    return 0\n",
    "\n",
    "similarities = []\n",
    "for rev2 in dataset:\n",
    "    tf = defaultdict(int)\n",
    "    r = rev2['review_text']\n",
    "    for w in r.split():\n",
    "        # Note = rather than +=\n",
    "        tf[w] = 1\n",
    "    tfidf2 = [tf[w] * math.log2(len(dataset) / df[w]) for w in words]\n",
    "    similarities.append((Cosine(tfidfQuery, tfidf2), rev2['review_text']))\n",
    "\n",
    "similarities.sort(reverse=True)\n",
    "sim, review = similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 'I checked this out of the library as their Valentine\\'s Day \"Blind Date with a Book\" display, and I\\'m glad I hooked up with this graphic novel. It\\'s been decades since I read any of King\\'s Dark Tower books, and although this jumped into the plot, I was able to catch up and enjoy this slice of the story. \\n The art is really nice, a the narration/dialogue balance works. It made me want to re-read King\\'s books, and it definitely made me excited for the upcoming film.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q2'] = [sim, review]\n",
    "answers['Q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q2']) == 2\n",
    "assertFloat(answers['Q2'][0])\n",
    "assert type(answers['Q2'][1]) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    reviewsPerUser[d['user_id']].append((dateutil.parser.parse(d['date_added']), d['book_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewLists = []\n",
    "for u in reviewsPerUser:\n",
    "    rl = list(reviewsPerUser[u])\n",
    "    rl.sort()\n",
    "    reviewLists.append([x[1] for x in rl])\n",
    "\n",
    "model10 = Word2Vec(reviewLists,\n",
    "                 min_count=1, # Words/items with fewer instances are discarded\n",
    "                 vector_size=10, # Model dimensionality\n",
    "                 window=3, # Window size\n",
    "                 sg=1) # Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18471619'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewLists[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8497638', 0.9409928917884827),\n",
       " ('25032624', 0.9308857321739197),\n",
       " ('21519210', 0.8949795365333557),\n",
       " ('22752448', 0.8882659673690796),\n",
       " ('5497136', 0.8640972375869751)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = model10.wv.similar_by_word(reviewLists[0][0])[:5]\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8497638', 0.9409928917884827),\n",
       " ('25032624', 0.9308857321739197),\n",
       " ('21519210', 0.8949795365333557),\n",
       " ('22752448', 0.8882659673690796),\n",
       " ('5497136', 0.8640972375869751)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q3'] = similarities # probably want model10.wv.similar_by_word(...)[:5]\n",
    "answers['Q3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q3']) == 5\n",
    "assert [type(x[0]) for x in answers['Q3']] == [str]*5\n",
    "assertFloatList([x[1] for x in answers['Q3']], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([d['rating'] for d in dataset]) / len(dataset)\n",
    "\n",
    "bookAverages = defaultdict(list)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "    \n",
    "for d in dataset:\n",
    "    b = d['book_id']\n",
    "    u = d['user_id']\n",
    "    bookAverages[b].append(d['rating'])\n",
    "    reviewsPerUser[u].append(d)\n",
    "    \n",
    "for b in bookAverages:\n",
    "    bookAverages[b] = sum(bookAverages[b]) / len(bookAverages[b])\n",
    "\n",
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    if not str(item) in model10.wv:\n",
    "        return ratingMean\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - bookAverages[i2])\n",
    "        if str(i2) in model10.wv:\n",
    "            similarities.append(model10.wv.distance(str(item), str(i2)))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return bookAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [predictRating(d['user_id'],d['book_id']) for d in dataset]\n",
    "labels = [d['rating'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse4 = MSE(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43403542771743414"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['Q4'] = mse4\n",
    "answers['Q4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerItem = defaultdict(list)\n",
    "for d in dataset:\n",
    "    reviewsPerItem[d['book_id']].append((dateutil.parser.parse(d['date_added']), d['user_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewLists = []\n",
    "for i in reviewsPerItem:\n",
    "    rl = list(reviewsPerItem[i])\n",
    "    rl.sort()\n",
    "    reviewLists.append([x[1] for x in rl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(reviewLists,\n",
    "                 min_count=1, # Words/items with fewer instances are discarded\n",
    "                 vector_size=10, # Model dimensionality\n",
    "                 window=3, # Window size\n",
    "                 sg=1) # Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dc3763cdb9b2cae805882878eebb6a32', 'eaa54d876d841293059657fb80a9bba6']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewLists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('efeb828154cb6024ae06501b582915fb', 0.9248121380805969),\n",
       " ('6bc8beca6151bf938e83d506e17c30e6', 0.8022053241729736),\n",
       " ('83cb3d94e518148de88b5873fc26d89c', 0.800977349281311),\n",
       " ('aa233705616224ecdeb8222f31403d02', 0.7624194025993347),\n",
       " ('b5e5c3932577b12cba9f3e84d029ddc3', 0.7350650429725647),\n",
       " ('74b6e3f6ec6370dd790b695198b0ea1b', 0.7223799228668213),\n",
       " ('1aaa886facd8dcd72be8f224bdd57ace', 0.7197946310043335),\n",
       " ('b9696fd94e05afe6df130405f2c4e056', 0.7197611331939697),\n",
       " ('db1737a0303404b846a7142fcd3bb05d', 0.7026280760765076),\n",
       " ('fa0a93ee673955268086e6b61a4b03ee', 0.7003586888313293)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(reviewLists[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([d['rating'] for d in dataset]) / len(dataset)\n",
    "\n",
    "userAverages = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "    \n",
    "for d in dataset:\n",
    "    b = d['book_id']\n",
    "    u = d['user_id']\n",
    "    userAverages[u].append(d['rating'])\n",
    "    reviewsPerItem[b].append(d)\n",
    "    \n",
    "for u in userAverages:\n",
    "    userAverages[u] = sum(userAverages[u]) / len(userAverages[u])\n",
    "\n",
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    if not str(user) in model.wv:\n",
    "        return ratingMean\n",
    "    for d in reviewsPerItem[item]:\n",
    "        u2 = d['user_id']\n",
    "        if u2 == user: continue\n",
    "        ratings.append(d['rating'] - userAverages[u2])\n",
    "        if str(u2) in model.wv:\n",
    "            similarities.append(model.wv.distance(str(user), str(u2)))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return userAverages[user] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [predictRating(d['user_id'],d['book_id']) for d in dataset]\n",
    "labels = [d['rating'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2322892157283016"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse5 = MSE(predictions, labels)\n",
    "mse5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    reviewsPerUser[d['user_id']].append((dateutil.parser.parse(d['date_added']), d['book_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewLists = []\n",
    "for u in reviewsPerUser:\n",
    "    rl = list(reviewsPerUser[u])\n",
    "    rl.sort()\n",
    "    reviewLists.append([x[1] for x in rl])\n",
    "\n",
    "model10 = Word2Vec(reviewLists,\n",
    "                 min_count=1, # Words/items with fewer instances are discarded\n",
    "                 vector_size=10, # Model dimensionality\n",
    "                 window=3, # Window size\n",
    "                 sg=5) # Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18471619']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewLists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('8497638', 0.9406467080116272),\n",
       " ('25032624', 0.9298714399337769),\n",
       " ('21519210', 0.8964695334434509),\n",
       " ('22752448', 0.8873355388641357),\n",
       " ('5497136', 0.8650296926498413)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = model10.wv.similar_by_word(reviewLists[0][0])[:5]\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([d['rating'] for d in dataset]) / len(dataset)\n",
    "\n",
    "bookAverages = defaultdict(list)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "    \n",
    "for d in dataset:\n",
    "    b = d['book_id']\n",
    "    u = d['user_id']\n",
    "    bookAverages[b].append(d['rating'])\n",
    "    reviewsPerUser[u].append(d)\n",
    "    \n",
    "for b in bookAverages:\n",
    "    bookAverages[b] = sum(bookAverages[b]) / len(bookAverages[b])\n",
    "\n",
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    if not str(item) in model10.wv:\n",
    "        return ratingMean\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - bookAverages[i2])\n",
    "        if str(i2) in model10.wv:\n",
    "            similarities.append(model10.wv.distance(str(item), str(i2)))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return bookAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [predictRating(d['user_id'],d['book_id']) for d in dataset]\n",
    "labels = [d['rating'] for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43398476240642303"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse5 = MSE(predictions, labels)\n",
    "mse5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = [\"I change the parameters sg(Skip-gram) of model from 1 to 5. The skip-gram model builds a model by predicting surrounding words given the current word.\",\n",
    "                 mse5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q5']) == 2\n",
    "assert type(answers['Q5'][0]) == str\n",
    "assertFloat(answers['Q5'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw4.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
